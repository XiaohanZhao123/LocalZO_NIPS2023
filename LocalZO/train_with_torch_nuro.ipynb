{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Serious of transformations using tonic\n",
    "* Step1: load from the tonic.datasets.Dataset\n",
    "* Step2: apply transformation defined in tonic.transforms, like Denoise, ToFrame\n",
    "* Step3: warp the dataset using a CachedDataset, which will cache the transformed data to disk\n",
    "* Step4: apply transformation to the frame (output from ToFrame), here we can use torch and torchvision transforms\n",
    "* Step5: warp the dataset using dataloader, but be aware of collate_fn, where we need to pad the frame to the same length\n",
    "* Step6: check if the result has shape [time, batch, channel, height, width] according to argument 'batch_first' in collate_fn, make sure we have time-first dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "array([(10, 30,    937, 1), (33, 20,   1030, 1), (12, 27,   1052, 1), ...,\n       ( 7, 15, 302706, 1), (26, 11, 303852, 1), (11, 17, 305341, 1)],\n      dtype=[('x', '<i8'), ('y', '<i8'), ('t', '<i8'), ('p', '<i8')])"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "import tonic\n",
    "\n",
    "# download the dataset from tonic\n",
    "dataset = tonic.datasets.NMNIST(save_to='./data', train=True)\n",
    "events, target = dataset[0]\n",
    "events"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tonic.transforms as transforms\n",
    "\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "# remove isolated events\n",
    "# sum a period of events into a frame\n",
    "frame_transform = transforms.Compose([\n",
    "    transforms.Denoise(filter_time=10000),\n",
    "    transforms.ToFrame(sensor_size=sensor_size, time_window=1000),\n",
    "])\n",
    "trainset = tonic.datasets.NMNIST(save_to='./data', train=True, transform=frame_transform)\n",
    "testset = tonic.datasets.NMNIST(save_to='./data', train=False, transform=frame_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([128, 312, 2, 34, 34])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tonic import DiskCachedDataset\n",
    "\n",
    "# then load the dataset the cache to accelerate data loading\n",
    "cached_trainset = DiskCachedDataset(trainset, cache_path='./data/cache')\n",
    "cached_testset = DiskCachedDataset(testset, cache_path='./data/cache')\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(cached_trainset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          collate_fn=tonic.collation.PadTensors())\n",
    "test_loader = DataLoader(cached_testset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=4,\n",
    "                            collate_fn=tonic.collation.PadTensors())\n",
    "\n",
    "# check the shape of input\n",
    "next(iter(train_loader))[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([311, 128, 2, 34, 34])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import functools\n",
    "\n",
    "transform = tonic.transforms.Compose([\n",
    "    torch.from_numpy,\n",
    "    torchvision.transforms.RandomRotation([-10,10]),\n",
    "])\n",
    "\n",
    "cached_trainset = DiskCachedDataset(trainset, cache_path='./data/cache', transform=transform)\n",
    "cached_testset = DiskCachedDataset(testset, cache_path='./data/cache',)\n",
    "\n",
    "# here, we need to pad the frame to the same length by using collate_fn\n",
    "# we then make time-first dataset by setting batch_first=False\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(cached_trainset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "\n",
    "test_loader = DataLoader(cached_testset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        num_workers=4,\n",
    "                        collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "\n",
    "\n",
    "# check the shape of input\n",
    "next(iter(train_loader))[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# define the network\n",
    "import snntorch as snn\n",
    "from snntorch import utils\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as F\n",
    "from snntorch import spikeplot as splt\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# define the forward pass\n",
    "\n",
    "def forward(net, data):\n",
    "    spk_rec = []\n",
    "    utils.reset(net) # reset the membrane potential of the network\n",
    "\n",
    "    for step in range(data.size(0)):\n",
    "        spk_out, mem_out = net(data[step])\n",
    "        spk_rec.append(spk_out)  # collect spike output\n",
    "\n",
    "    return torch.stack(spk_rec, dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([12, 2, 5, 5])\n",
      "conv1.bias torch.Size([12])\n",
      "conv2.weight torch.Size([32, 12, 5, 5])\n",
      "conv2.bias torch.Size([32])\n",
      "linear.weight torch.Size([10, 800])\n",
      "linear.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# neuron and simulation parameters\n",
    "spike_grad = surrogate.atan()\n",
    "beta = 0.5\n",
    "\n",
    "#  Initialize Network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 12, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.snn1 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "        self.conv2 = nn.Conv2d(12, 32, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.snn2 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(32*5*5, 10)\n",
    "        self.snn3 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.snn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.snn2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.snn3(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_sparsity(x):\n",
    "        return (x == 0).float().mean()\n",
    "\n",
    "net = SimpleNet().to(device)\n",
    "for k, v in net.named_parameters():\n",
    "    print(k, v.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# define the loss function and optimizer\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-2)\n",
    "\n",
    "# here we use snntorch.functional's loss function to accumulate the loss\n",
    "loss_fn = F.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0 \n",
      "Train Loss: 31.00, time: 2.40s\n",
      "Accuracy: 8.59%\n",
      "\n",
      "Epoch 0, Iteration 1 \n",
      "Train Loss: 30.96, time: 1.12s\n",
      "Accuracy: 7.81%\n",
      "\n",
      "Epoch 0, Iteration 2 \n",
      "Train Loss: 30.90, time: 1.07s\n",
      "Accuracy: 10.94%\n",
      "\n",
      "Epoch 0, Iteration 3 \n",
      "Train Loss: 30.93, time: 1.05s\n",
      "Accuracy: 8.59%\n",
      "\n",
      "Epoch 0, Iteration 4 \n",
      "Train Loss: 18.26, time: 1.11s\n",
      "Accuracy: 14.06%\n",
      "\n",
      "Epoch 0, Iteration 5 \n",
      "Train Loss: 12.85, time: 1.14s\n",
      "Accuracy: 16.41%\n",
      "\n",
      "Epoch 0, Iteration 6 \n",
      "Train Loss: 16.11, time: 1.10s\n",
      "Accuracy: 11.72%\n",
      "\n",
      "Epoch 0, Iteration 7 \n",
      "Train Loss: 16.39, time: 1.08s\n",
      "Accuracy: 18.75%\n",
      "\n",
      "Epoch 0, Iteration 8 \n",
      "Train Loss: 15.91, time: 1.21s\n",
      "Accuracy: 19.53%\n",
      "\n",
      "Epoch 0, Iteration 9 \n",
      "Train Loss: 13.14, time: 1.12s\n",
      "Accuracy: 28.12%\n",
      "\n",
      "Epoch 0, Iteration 10 \n",
      "Train Loss: 13.31, time: 1.18s\n",
      "Accuracy: 29.69%\n",
      "\n",
      "Epoch 0, Iteration 11 \n",
      "Train Loss: 13.80, time: 1.10s\n",
      "Accuracy: 25.78%\n",
      "\n",
      "Epoch 0, Iteration 12 \n",
      "Train Loss: 12.76, time: 1.21s\n",
      "Accuracy: 28.91%\n",
      "\n",
      "Epoch 0, Iteration 13 \n",
      "Train Loss: 12.23, time: 1.12s\n",
      "Accuracy: 24.22%\n",
      "\n",
      "Epoch 0, Iteration 14 \n",
      "Train Loss: 12.29, time: 1.06s\n",
      "Accuracy: 34.38%\n",
      "\n",
      "Epoch 0, Iteration 15 \n",
      "Train Loss: 13.15, time: 1.06s\n",
      "Accuracy: 31.25%\n",
      "\n",
      "Epoch 0, Iteration 16 \n",
      "Train Loss: 12.71, time: 1.64s\n",
      "Accuracy: 28.12%\n",
      "\n",
      "Epoch 0, Iteration 17 \n",
      "Train Loss: 11.75, time: 1.17s\n",
      "Accuracy: 28.12%\n",
      "\n",
      "Epoch 0, Iteration 18 \n",
      "Train Loss: 11.65, time: 1.09s\n",
      "Accuracy: 27.34%\n",
      "\n",
      "Epoch 0, Iteration 19 \n",
      "Train Loss: 11.78, time: 1.13s\n",
      "Accuracy: 27.34%\n",
      "\n",
      "Epoch 0, Iteration 20 \n",
      "Train Loss: 11.22, time: 1.12s\n",
      "Accuracy: 36.72%\n",
      "\n",
      "Epoch 0, Iteration 21 \n",
      "Train Loss: 10.53, time: 1.17s\n",
      "Accuracy: 37.50%\n",
      "\n",
      "Epoch 0, Iteration 22 \n",
      "Train Loss: 10.35, time: 1.10s\n",
      "Accuracy: 50.00%\n",
      "\n",
      "Epoch 0, Iteration 23 \n",
      "Train Loss: 10.01, time: 1.21s\n",
      "Accuracy: 49.22%\n",
      "\n",
      "Epoch 0, Iteration 24 \n",
      "Train Loss: 9.87, time: 1.25s\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Epoch 0, Iteration 25 \n",
      "Train Loss: 9.27, time: 1.23s\n",
      "Accuracy: 57.03%\n",
      "\n",
      "Epoch 0, Iteration 26 \n",
      "Train Loss: 9.48, time: 1.10s\n",
      "Accuracy: 48.44%\n",
      "\n",
      "Epoch 0, Iteration 27 \n",
      "Train Loss: 9.29, time: 1.11s\n",
      "Accuracy: 61.72%\n",
      "\n",
      "Epoch 0, Iteration 28 \n",
      "Train Loss: 9.46, time: 1.15s\n",
      "Accuracy: 57.03%\n",
      "\n",
      "Epoch 0, Iteration 29 \n",
      "Train Loss: 9.18, time: 1.13s\n",
      "Accuracy: 57.03%\n",
      "\n",
      "Epoch 0, Iteration 30 \n",
      "Train Loss: 9.04, time: 1.10s\n",
      "Accuracy: 63.28%\n",
      "\n",
      "Epoch 0, Iteration 31 \n",
      "Train Loss: 8.65, time: 1.20s\n",
      "Accuracy: 64.06%\n",
      "\n",
      "Epoch 0, Iteration 32 \n",
      "Train Loss: 8.24, time: 1.18s\n",
      "Accuracy: 65.62%\n",
      "\n",
      "Epoch 0, Iteration 33 \n",
      "Train Loss: 8.61, time: 1.12s\n",
      "Accuracy: 57.81%\n",
      "\n",
      "Epoch 0, Iteration 34 \n",
      "Train Loss: 8.28, time: 1.08s\n",
      "Accuracy: 59.38%\n",
      "\n",
      "Epoch 0, Iteration 35 \n",
      "Train Loss: 8.20, time: 1.08s\n",
      "Accuracy: 55.47%\n",
      "\n",
      "Epoch 0, Iteration 36 \n",
      "Train Loss: 7.84, time: 1.12s\n",
      "Accuracy: 60.94%\n",
      "\n",
      "Epoch 0, Iteration 37 \n",
      "Train Loss: 7.78, time: 1.16s\n",
      "Accuracy: 66.41%\n",
      "\n",
      "Epoch 0, Iteration 38 \n",
      "Train Loss: 7.12, time: 1.14s\n",
      "Accuracy: 67.97%\n",
      "\n",
      "Epoch 0, Iteration 39 \n",
      "Train Loss: 7.49, time: 1.23s\n",
      "Accuracy: 61.72%\n",
      "\n",
      "Epoch 0, Iteration 40 \n",
      "Train Loss: 7.09, time: 1.20s\n",
      "Accuracy: 75.78%\n",
      "\n",
      "Epoch 0, Iteration 41 \n",
      "Train Loss: 7.23, time: 1.17s\n",
      "Accuracy: 64.84%\n",
      "\n",
      "Epoch 0, Iteration 42 \n",
      "Train Loss: 6.91, time: 1.13s\n",
      "Accuracy: 73.44%\n",
      "\n",
      "Epoch 0, Iteration 43 \n",
      "Train Loss: 7.17, time: 1.16s\n",
      "Accuracy: 70.31%\n",
      "\n",
      "Epoch 0, Iteration 44 \n",
      "Train Loss: 6.61, time: 1.19s\n",
      "Accuracy: 72.66%\n",
      "\n",
      "Epoch 0, Iteration 45 \n",
      "Train Loss: 6.35, time: 1.17s\n",
      "Accuracy: 75.78%\n",
      "\n",
      "Epoch 0, Iteration 46 \n",
      "Train Loss: 7.12, time: 1.11s\n",
      "Accuracy: 64.84%\n",
      "\n",
      "Epoch 0, Iteration 47 \n",
      "Train Loss: 6.80, time: 1.08s\n",
      "Accuracy: 68.75%\n",
      "\n",
      "Epoch 0, Iteration 48 \n",
      "Train Loss: 6.55, time: 1.32s\n",
      "Accuracy: 70.31%\n",
      "\n",
      "Epoch 0, Iteration 49 \n",
      "Train Loss: 6.32, time: 1.17s\n",
      "Accuracy: 75.00%\n",
      "\n",
      "Epoch 0, Iteration 50 \n",
      "Train Loss: 5.86, time: 1.13s\n",
      "Accuracy: 71.09%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 1\n",
    "num_iters = 50\n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
    "        start = time.time()\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        spk_rec = forward(net, data)\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}, time: {end-start:.2f}s\")\n",
    "\n",
    "        # measure the acc with rate coding\n",
    "        acc = F.accuracy_rate(spk_rec, targets)\n",
    "        acc_hist.append(acc)\n",
    "        print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
    "\n",
    "        # training loop breaks after 50 iterations\n",
    "        if i == num_iters:\n",
    "          break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-.conda-xiaohan-py",
   "language": "python",
   "display_name": "Python [conda env:.conda-xiaohan] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}