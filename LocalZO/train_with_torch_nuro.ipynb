{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Serious of transformations using tonic\n",
    "* Step1: load from the tonic.datasets.Dataset\n",
    "* Step2: apply transformation defined in tonic.transforms, like Denoise, ToFrame\n",
    "* Step3: warp the dataset using a CachedDataset, which will cache the transformed data to disk\n",
    "* Step4: apply transformation to the frame (output from ToFrame), here we can use torch and torchvision transforms\n",
    "* Step5: warp the dataset using dataloader, but be aware of collate_fn, where we need to pad the frame to the same length\n",
    "* Step6: check if the result has shape [time, batch, channel, height, width] according to argument 'batch_first' in collate_fn, make sure we have time-first dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "array([(10, 30,    937, 1), (33, 20,   1030, 1), (12, 27,   1052, 1), ...,\n       ( 7, 15, 302706, 1), (26, 11, 303852, 1), (11, 17, 305341, 1)],\n      dtype=[('x', '<i8'), ('y', '<i8'), ('t', '<i8'), ('p', '<i8')])"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "import tonic\n",
    "\n",
    "# download the dataset from tonic\n",
    "dataset = tonic.datasets.NMNIST(save_to='/home/zxh/data', train=True)\n",
    "events, target = dataset[0]\n",
    "events"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tonic.transforms as transforms\n",
    "\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "# remove isolated events\n",
    "# sum a period of events into a frame\n",
    "frame_transform = transforms.Compose([\n",
    "    transforms.Denoise(filter_time=10000),\n",
    "    transforms.ToFrame(sensor_size=sensor_size, time_window=1000),\n",
    "])\n",
    "trainset = tonic.datasets.NMNIST(save_to='/home/zxh/data', train=True, transform=frame_transform)\n",
    "testset = tonic.datasets.NMNIST(save_to='/home/zxh/data', train=False, transform=frame_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([128, 310, 2, 34, 34])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tonic import DiskCachedDataset\n",
    "\n",
    "# then load the dataset the cache to accelerate data loading\n",
    "cached_trainset = DiskCachedDataset(trainset, cache_path='./data/cache')\n",
    "cached_testset = DiskCachedDataset(testset, cache_path='./data/cache')\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(cached_trainset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          collate_fn=tonic.collation.PadTensors())\n",
    "test_loader = DataLoader(cached_testset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=4,\n",
    "                            collate_fn=tonic.collation.PadTensors())\n",
    "\n",
    "# check the shape of input\n",
    "next(iter(train_loader))[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([310, 128, 2, 34, 34])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import functools\n",
    "\n",
    "transform = tonic.transforms.Compose([\n",
    "    torch.from_numpy,\n",
    "    torchvision.transforms.RandomRotation([-10,10]),\n",
    "])\n",
    "\n",
    "cached_trainset = DiskCachedDataset(trainset, cache_path='./data/cache', transform=transform)\n",
    "cached_testset = DiskCachedDataset(testset, cache_path='./data/cache',)\n",
    "\n",
    "# here, we need to pad the frame to the same length by using collate_fn\n",
    "# we then make time-first dataset by setting batch_first=False\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(cached_trainset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "\n",
    "test_loader = DataLoader(cached_testset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        num_workers=4,\n",
    "                        collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "\n",
    "\n",
    "# check the shape of input\n",
    "next(iter(train_loader))[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# define the network\n",
    "import snntorch as snn\n",
    "from snntorch import utils\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as F\n",
    "from snntorch import spikeplot as splt\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# define the forward pass\n",
    "\n",
    "def forward(net, data):\n",
    "    spk_rec = []\n",
    "    utils.reset(net) # reset the membrane potential of the network\n",
    "\n",
    "    for step in range(data.size(0)):\n",
    "        spk_out, mem_out = net(data[step])\n",
    "        spk_rec.append(spk_out)  # collect spike output\n",
    "\n",
    "    return torch.stack(spk_rec, dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([12, 2, 5, 5])\n",
      "conv1.bias torch.Size([12])\n",
      "bn1.weight torch.Size([12])\n",
      "bn1.bias torch.Size([12])\n",
      "conv2.weight torch.Size([32, 12, 5, 5])\n",
      "conv2.bias torch.Size([32])\n",
      "bn2.weight torch.Size([32])\n",
      "bn2.bias torch.Size([32])\n",
      "linear.weight torch.Size([10, 800])\n",
      "linear.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# neuron and simulation parameters\n",
    "spike_grad = surrogate.sigmoid()\n",
    "beta = 0.5\n",
    "\n",
    "#  Initialize Network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 12, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.bn1 = nn.BatchNorm2d(12)\n",
    "        self.snn1 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "        self.conv2 = nn.Conv2d(12, 32, 5)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.snn2 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(32*5*5, 10)\n",
    "        self.snn3 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.snn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.snn2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.snn3(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_sparsity(x):\n",
    "        return (x == 0).float().mean()\n",
    "\n",
    "net = SimpleNet().to(device)\n",
    "for k, v in net.named_parameters():\n",
    "    print(k, v.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# define the loss function and optimizer\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3)\n",
    "\n",
    "# here we use snntorch.functional's loss function to accumulate the loss\n",
    "loss_fn = F.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0 \n",
      "Train Loss: 30.50, time: 7.42s\n",
      "Accuracy: 5.47%\n",
      "\n",
      "Epoch 0, Iteration 1 \n",
      "Train Loss: 30.90, time: 2.86s\n",
      "Accuracy: 9.38%\n",
      "\n",
      "Epoch 0, Iteration 2 \n",
      "Train Loss: 31.00, time: 2.34s\n",
      "Accuracy: 11.72%\n",
      "\n",
      "Epoch 0, Iteration 3 \n",
      "Train Loss: 31.00, time: 2.13s\n",
      "Accuracy: 8.59%\n",
      "\n",
      "Epoch 0, Iteration 4 \n",
      "Train Loss: 31.00, time: 2.49s\n",
      "Accuracy: 10.16%\n",
      "\n",
      "Epoch 0, Iteration 5 \n",
      "Train Loss: 30.90, time: 2.82s\n",
      "Accuracy: 10.94%\n",
      "\n",
      "Epoch 0, Iteration 6 \n",
      "Train Loss: 30.90, time: 2.36s\n",
      "Accuracy: 13.28%\n",
      "\n",
      "Epoch 0, Iteration 7 \n",
      "Train Loss: 30.90, time: 2.85s\n",
      "Accuracy: 14.84%\n",
      "\n",
      "Epoch 0, Iteration 8 \n",
      "Train Loss: 30.90, time: 2.03s\n",
      "Accuracy: 7.03%\n",
      "\n",
      "Epoch 0, Iteration 9 \n",
      "Train Loss: 30.90, time: 2.51s\n",
      "Accuracy: 10.94%\n",
      "\n",
      "Epoch 0, Iteration 10 \n",
      "Train Loss: 31.00, time: 2.54s\n",
      "Accuracy: 7.81%\n",
      "\n",
      "Epoch 0, Iteration 11 \n",
      "Train Loss: 31.00, time: 2.49s\n",
      "Accuracy: 9.38%\n",
      "\n",
      "Epoch 0, Iteration 12 \n",
      "Train Loss: 30.58, time: 2.88s\n",
      "Accuracy: 8.59%\n",
      "\n",
      "Epoch 0, Iteration 13 \n",
      "Train Loss: 30.96, time: 2.54s\n",
      "Accuracy: 10.94%\n",
      "\n",
      "Epoch 0, Iteration 14 \n",
      "Train Loss: 30.58, time: 2.52s\n",
      "Accuracy: 10.94%\n",
      "\n",
      "Epoch 0, Iteration 15 \n",
      "Train Loss: 30.96, time: 2.25s\n",
      "Accuracy: 8.59%\n",
      "\n",
      "Epoch 0, Iteration 16 \n",
      "Train Loss: 31.08, time: 2.67s\n",
      "Accuracy: 12.50%\n",
      "\n",
      "Epoch 0, Iteration 17 \n",
      "Train Loss: 31.00, time: 2.09s\n",
      "Accuracy: 7.81%\n",
      "\n",
      "Epoch 0, Iteration 18 \n",
      "Train Loss: 31.02, time: 2.18s\n",
      "Accuracy: 9.38%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 1\n",
    "num_iters = 51\n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    time_list = []\n",
    "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
    "        start = time.time()\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        spk_rec = forward(net, data)\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        if not i == 0:\n",
    "            time_list.append(end-start) # skip first for warmup\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}, time: {end-start:.2f}s\")\n",
    "\n",
    "        # measure the acc with rate coding\n",
    "        acc = F.accuracy_rate(spk_rec, targets)\n",
    "        acc_hist.append(acc)\n",
    "        print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
    "\n",
    "        # training loop breaks after 50 iterations\n",
    "        if i == num_iters:\n",
    "          break\n",
    "    print(f\"Average time per iteration: {sum(time_list)/len(time_list):.2f}s\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-.conda-xiaohan-py",
   "language": "python",
   "display_name": "Python [conda env:.conda-xiaohan] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}