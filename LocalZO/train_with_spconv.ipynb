{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training the Spconv\n",
    "This tutorial aims at twofold: First, we introduce how to construct and train network with spconv and how to glue the SNN neurons and spconv layers together. Second, we introduce how replace the snn neurons using surrogate gradient with Local ZO neurons.\n",
    "## 1 Introduction for Components\n",
    "### 1.1 Spconv\n",
    "Spconv are a pytorch library for sparse convolution, we use for conv layers, batch norm layers and pooling layers.\n",
    "GitHub homepage: https://github.com/traveller59/spconv\n",
    "### 1.2 LocalZO.conv_models\n",
    "Implementations of Spike Neurons (So far, only LIF), that can glue between Spconv layers\n",
    "\n",
    "## 2 Training the Spconv\n",
    "### 2.1 Data loading\n",
    "first, let's load the data using tonic. Details can be found in train_with_torch_nuro.ipynb, so we just skip it here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size: 60000 testset size: 10000 type of dataset <class 'numpy.ndarray'> shape (298, 2, 34, 34)\n"
     ]
    }
   ],
   "source": [
    "# set the gpu device\n",
    "gpu_idx = '3'\n",
    "import os\n",
    "from tonic.datasets import NMNIST\n",
    "import tonic\n",
    "from tonic import transforms\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_idx\n",
    "\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "# remove isolated events\n",
    "# sum a period of events into a frame\n",
    "frame_transform = transforms.Compose([\n",
    "    transforms.Denoise(filter_time=10000),\n",
    "    transforms.ToFrame(sensor_size=sensor_size, time_window=1000),\n",
    "])  # the output of ToFrame is a tuple of (frame, label), where frame is np.ndarray\n",
    "\n",
    "trainset = tonic.datasets.NMNIST(save_to='/home/zxh/data', train=True, transform=frame_transform)\n",
    "testset = tonic.datasets.NMNIST(save_to='/home/zxh/data', train=False, transform=frame_transform)\n",
    "print('trainset size:', len(trainset), 'testset size:', len(testset), 'type of dataset', type(trainset[0][0]), 'shape', trainset[0][0].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Define the network\n",
    "The construction of a Spconv network is basically same with constructing any torch.nn networks, except for a few things to notice:\n",
    "* The computation is time-first, and at the end of LIF layer we reshape the input into [time*batch, *inputs] before feeding into spconv network to improve the performance. So, we need to reshape it back before output.\n",
    "* The input of a Spconv network is a sparse tensor, constructed by spconv.SparseConvTensor.from_dense()\n",
    "* The LeakyPlain (implemented in LocalZO.conv_models.neurons) is a spike neuron that can glue between Spconv layers. It needs to specify the batch_size, u_th and beta. Batch size is used to reshape input and output, u_th and beta are parameters of LIF neuron.\n",
    "* The output should be a tensor with shape (time, batch, *inputs).\n",
    "\n",
    "**Important:** The spconv library treat empty input (i.e. input with only zeros) as an error\n",
    "Usually this will not happen, if so, we will get *CUDA kernel launch blocks must be positive, but got N= 0*. and please set appropriate u_th and beta to make at least one spike fire during the time stamps. Details can be found in : https://github.com/traveller59/spconv/blob/master/docs/COMMON_PROBLEMS.md"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExampleNet(\n",
      "  (conv_block1): Sequential(\n",
      "    (0): SparseConv2d(2, 16, kernel_size=[5, 5], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.Native)\n",
      "    (1): SparseBatchNorm(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SparseMaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=[0, 0], dilation=[1, 1], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (3): LeakyPlain()\n",
      "  )\n",
      "  (conv_block2): Sequential(\n",
      "    (0): SparseConv2d(16, 32, kernel_size=[5, 5], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.Native)\n",
      "    (1): SparseBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SparseMaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=[0, 0], dilation=[1, 1], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (3): LeakyPlain()\n",
      "  )\n",
      "  (to_dense): ToDense()\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=800, out_features=10, bias=True)\n",
      "    (1): LeakyPlain()\n",
      "  )\n",
      ")\n",
      "conv_block1.0.weight torch.Size([16, 5, 5, 2])\n",
      "conv_block1.0.bias torch.Size([16])\n",
      "conv_block1.1.weight torch.Size([16])\n",
      "conv_block1.1.bias torch.Size([16])\n",
      "conv_block2.0.weight torch.Size([32, 5, 5, 16])\n",
      "conv_block2.0.bias torch.Size([32])\n",
      "conv_block2.1.weight torch.Size([32])\n",
      "conv_block2.1.bias torch.Size([32])\n",
      "fc.0.weight torch.Size([10, 800])\n",
      "fc.0.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from spconv import pytorch as spconv  # pay attention to the spconv.pytorch\n",
    "from conv_models.neurons import LeakyPlain\n",
    "\n",
    "\n",
    "class ExampleNet(nn.Module):\n",
    "    def __init__(self, batch_size, u_th, beta, conv_algorithm = spconv.ConvAlgo.Native):\n",
    "        super(ExampleNet, self).__init__()\n",
    "        self.batch_size= batch_size\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            spconv.SparseConv2d(2, 16, 5, bias=True, algo=conv_algorithm),\n",
    "            spconv.SparseBatchNorm(16, eps=1e-5, momentum=0.1),\n",
    "            spconv.SparseMaxPool2d(2, stride=2),\n",
    "            LeakyPlain(u_th=u_th, beta=beta, batch_size=batch_size),\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            spconv.SparseConv2d(16, 32, 5, bias=True, algo=conv_algorithm),\n",
    "            spconv.SparseBatchNorm(32, eps=1e-5, momentum=0.1),\n",
    "            spconv.SparseMaxPool2d(2, stride=2),\n",
    "            LeakyPlain(u_th=u_th, beta=beta, batch_size=batch_size),\n",
    "        )\n",
    "        self.to_dense = spconv.ToDense() # convert the sparse tensor to dense tensor\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32*5*5, 10),\n",
    "            LeakyPlain(u_th=u_th, beta=beta, batch_size=batch_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert isinstance(x, spconv.SparseConvTensor)\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.to_dense(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.batch_size, 10)  # reshape back into time, batch, *inputs\n",
    "        return x\n",
    "\n",
    "batch_size = 128\n",
    "net = ExampleNet(batch_size=batch_size, u_th=1.0, beta=0.5).cuda()\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Define the loss function and optimizer\n",
    "we use loss function from snntorch, and optimizer from torch.optim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from snntorch import functional as SF\n",
    "import torch\n",
    "\n",
    "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "acc_fn = SF.accuracy_rate\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Cache dataset to accelerate training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cache_transform = tonic.transforms.Compose([\n",
    "    torch.from_numpy,\n",
    "    torchvision.transforms.RandomRotation([-10,10]),\n",
    "])\n",
    "\n",
    "epoch_num = 5\n",
    "cached_trainset = tonic.DiskCachedDataset(trainset, cache_path='./data/cache', transform=cache_transform)\n",
    "cached_testset = tonic.DiskCachedDataset(testset, cache_path='./data/cache',)\n",
    "\n",
    "train_loader = DataLoader(cached_trainset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "test_loader = DataLoader(cached_testset, batch_size=batch_size, shuffle=False, num_workers=4, drop_last=True,\n",
    "                         collate_fn=tonic.collation.PadTensors(batch_first=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.6 Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 19:32:18.463947: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-03 19:32:20.531580: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-05-03 19:32:20.531835: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-05-03 19:32:20.531857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 batch 0 loss 28.434736251831055 acc 0.1015625 time 14.419233083724976\n",
      "epoch 0 batch 10 loss 10.476030349731445 acc 0.328125 time 1.4713521003723145\n",
      "epoch 0 batch 20 loss 8.578996658325195 acc 0.4921875 time 1.731255054473877\n",
      "epoch 0 batch 30 loss 7.9581074714660645 acc 0.5390625 time 1.2566273212432861\n",
      "epoch 0 batch 40 loss 7.049453258514404 acc 0.6171875 time 1.0027294158935547\n",
      "epoch 0 batch 50 loss 6.671371936798096 acc 0.6484375 time 2.2599339485168457\n",
      "epoch 0 batch 60 loss 6.267968654632568 acc 0.71875 time 0.9950437545776367\n",
      "epoch 0 batch 70 loss 6.827608108520508 acc 0.6875 time 1.298081636428833\n",
      "epoch 0 batch 80 loss 5.799502372741699 acc 0.765625 time 1.0090351104736328\n",
      "epoch 0 batch 90 loss 5.619458198547363 acc 0.796875 time 1.1825788021087646\n",
      "epoch 0 batch 100 loss 5.831307888031006 acc 0.7578125 time 1.4779713153839111\n",
      "epoch 0 batch 110 loss 5.420891284942627 acc 0.8515625 time 1.3453187942504883\n",
      "epoch 0 batch 120 loss 5.045302391052246 acc 0.8359375 time 1.346778392791748\n",
      "epoch 0 batch 130 loss 5.560008525848389 acc 0.78125 time 1.254011631011963\n",
      "epoch 0 batch 140 loss 5.286330699920654 acc 0.8125 time 1.157832145690918\n",
      "epoch 0 batch 150 loss 5.139798641204834 acc 0.7890625 time 1.2454757690429688\n",
      "epoch 0 batch 160 loss 5.296953201293945 acc 0.8359375 time 1.2064499855041504\n",
      "epoch 0 batch 170 loss 5.191380977630615 acc 0.7421875 time 1.2104156017303467\n",
      "epoch 0 batch 180 loss 4.399843692779541 acc 0.875 time 1.4368185997009277\n",
      "epoch 0 batch 190 loss 4.530149936676025 acc 0.859375 time 1.0479166507720947\n",
      "epoch 0 batch 200 loss 4.869172096252441 acc 0.828125 time 1.0998663902282715\n",
      "epoch 0 batch 210 loss 5.044315338134766 acc 0.7890625 time 1.0700180530548096\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/home/zxh/.conda/envs/xiaohan/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/zxh/.conda/envs/xiaohan/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/zxh/.conda/envs/xiaohan/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/zxh/.conda/envs/xiaohan/lib/python3.8/site-packages/tonic/cached_dataset.py\", line 137, in __getitem__\n    data, targets = load_from_disk_cache(file_path)\n  File \"/home/zxh/.conda/envs/xiaohan/lib/python3.8/site-packages/tonic/cached_dataset.py\", line 214, in load_from_disk_cache\n    for index in f[name].keys():\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/home/zxh/.conda/envs/xiaohan/lib/python3.8/site-packages/h5py/_hl/group.py\", line 328, in __getitem__\n    oid = h5o.open(self.id, self._e(name), lapl=self._lapl)\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py/h5o.pyx\", line 190, in h5py.h5o.open\nKeyError: \"Unable to open object (object 'target' doesn't exist)\"\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SummaryWriter(comment\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspconv\u001B[39m\u001B[38;5;124m'\u001B[39m, log_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./output/spconv\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m writer:\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epoch_num):\n\u001B[0;32m----> 6\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i, (inputs, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[1;32m      7\u001B[0m             start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m      8\u001B[0m             inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m*\u001B[39minputs\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m:])\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m)\u001B[38;5;241m.\u001B[39mcuda()\n",
      "File \u001B[0;32m~/.conda/envs/xiaohan/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/.conda/envs/xiaohan/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1326\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rcvd_idx]) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m   1325\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rcvd_idx)[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m-> 1326\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1328\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   1329\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_data()\n",
      "File \u001B[0;32m~/.conda/envs/xiaohan/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1372\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m   1370\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[1;32m   1371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[0;32m-> 1372\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/.conda/envs/xiaohan/lib/python3.8/site-packages/torch/_utils.py:644\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    640\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    641\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[1;32m    642\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m--> 644\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[0;31mKeyError\u001B[0m: Caught KeyError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/home/zxh/.conda/envs/xiaohan/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/zxh/.conda/envs/xiaohan/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/zxh/.conda/envs/xiaohan/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/zxh/.conda/envs/xiaohan/lib/python3.8/site-packages/tonic/cached_dataset.py\", line 137, in __getitem__\n    data, targets = load_from_disk_cache(file_path)\n  File \"/home/zxh/.conda/envs/xiaohan/lib/python3.8/site-packages/tonic/cached_dataset.py\", line 214, in load_from_disk_cache\n    for index in f[name].keys():\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/home/zxh/.conda/envs/xiaohan/lib/python3.8/site-packages/h5py/_hl/group.py\", line 328, in __getitem__\n    oid = h5o.open(self.id, self._e(name), lapl=self._lapl)\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py/h5o.pyx\", line 190, in h5py.h5o.open\nKeyError: \"Unable to open object (object 'target' doesn't exist)\"\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "with SummaryWriter(comment='spconv', log_dir='./output/spconv') as writer:\n",
    "    for epoch in range(epoch_num):\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            start = time.time()\n",
    "            inputs = inputs.view(-1, *inputs.shape[2:]).transpose(1, 3).cuda()\n",
    "            inputs = spconv.SparseConvTensor.from_dense(inputs)\n",
    "            labels = labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                writer.add_scalar('loss', loss.item(), epoch * len(train_loader) + i)\n",
    "                writer.add_scalar('acc', acc_fn(outputs, labels).item(), epoch * len(train_loader) + i)\n",
    "                print('epoch', epoch, 'batch', i, 'loss', loss.item(), 'acc', acc_fn(outputs, labels).item(),\n",
    "                      'time', end - start)\n",
    "\n",
    "        acc = 0\n",
    "        loss_val = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = net(inputs)\n",
    "            loss_val += loss_fn(outputs, labels).item()\n",
    "            acc += acc_fn(outputs, labels).item()\n",
    "\n",
    "        writer.add_scalar('test loss', loss_val/len(test_loader), epoch)\n",
    "        writer.add_scalar('test acc', acc/len(test_loader), epoch)\n",
    "        print('epoch', epoch, 'test loss', loss_val/len(test_loader), 'test acc', acc/len(test_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Training with LocalZO neurons\n",
    "### 3.1 Implementation Details\n",
    "The LocalZO neurons use random sampler to generate random tangents to approximate gradient\n",
    "Since the sampling technique varies, to instantiate a LocalZo neuron, we need to provide a sampler. For customized sampler, we recommend to inherit the base class `BaseSampler` and implement the `generate_random_tangents` method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from conv_models.neurons import LeakeyZOPlain\n",
    "from conv_models.samplers import NormalSampler, BaseSampler\n",
    "\n",
    "\n",
    "class ExampleNetLocalZO(nn.Module):\n",
    "    def __init__(self, batch_size, u_th, beta, conv_algorithm = spconv.ConvAlgo.Native, random_sampler: BaseSampler=NormalSampler, sample_num=1):\n",
    "        super(ExampleNetLocalZO, self).__init__()\n",
    "        self.batch_size= batch_size\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            spconv.SparseConv2d(2, 16, 5, bias=True, algo=conv_algorithm),\n",
    "            spconv.SparseBatchNorm(16, eps=1e-5, momentum=0.1),\n",
    "            spconv.SparseMaxPool2d(2, stride=2),\n",
    "            LeakeyZOPlain(u_th=u_th, beta=beta, batch_size=batch_size, random_sampler=random_sampler(),sample_num=sample_num)\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            spconv.SparseConv2d(16, 32, 5, bias=True, algo=conv_algorithm),\n",
    "            spconv.SparseBatchNorm(32, eps=1e-5, momentum=0.1),\n",
    "            spconv.SparseMaxPool2d(2, stride=2),\n",
    "            LeakeyZOPlain(u_th=u_th, beta=beta, batch_size=batch_size, random_sampler=random_sampler(),sample_num=sample_num)\n",
    "        )\n",
    "        self.to_dense = spconv.ToDense() # convert the sparse tensor to dense tensor\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32*5*5, 10),\n",
    "            LeakyPlain(u_th=u_th, beta=beta, batch_size=batch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert isinstance(x, spconv.SparseConvTensor)\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.to_dense(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.batch_size, 10)  # reshape back into time, batch, *inputs\n",
    "        return x\n",
    "\n",
    "net = ExampleNetLocalZO(batch_size=batch_size, u_th=1.0, beta=0.5).cuda()\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from snntorch import functional as SF\n",
    "import torch\n",
    "\n",
    "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "acc_fn = SF.accuracy_rate\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3)\n",
    "\n",
    "with SummaryWriter(comment='spconv', log_dir='./output/spconv') as writer:\n",
    "    for epoch in range(epoch_num):\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            start = time.time()\n",
    "            inputs = inputs.view(-1, *inputs.shape[2:]).transpose(1, 3).cuda()\n",
    "            inputs = spconv.SparseConvTensor.from_dense(inputs)\n",
    "            labels = labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                writer.add_scalar('loss', loss.item(), epoch * len(train_loader) + i)\n",
    "                writer.add_scalar('acc', acc_fn(outputs, labels).item(), epoch * len(train_loader) + i)\n",
    "                print('epoch', epoch, 'batch', i, 'loss', loss.item(), 'acc', acc_fn(outputs, labels).item(), 'time', end-start)\n",
    "\n",
    "        acc = 0\n",
    "        loss_val = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = net(inputs)\n",
    "            loss_val += loss_fn(outputs, labels).item()\n",
    "            acc += acc_fn(outputs, labels).item()\n",
    "\n",
    "        writer.add_scalar('test loss', loss_val/len(test_loader), epoch)\n",
    "        writer.add_scalar('test acc', acc/len(test_loader), epoch)\n",
    "        print('epoch', epoch, 'test loss', loss_val/len(test_loader), 'test acc', acc/len(test_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-.conda-xiaohan-py",
   "language": "python",
   "display_name": "Python [conda env:.conda-xiaohan] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}