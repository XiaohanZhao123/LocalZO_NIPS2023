{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training the Spconv\n",
    "This tutorial aims at twofold: First, we introduce how to construct and train network with spconv and how to glue the SNN neurons and spconv layers together. Second, we introduce how replace the snn neurons using surrogate gradient with Local ZO neurons.\n",
    "## 1 Introduction for Components\n",
    "### 1.1 Spconv\n",
    "Spconv are a pytorch library for sparse convolution, we use for conv layers, batch norm layers and pooling layers.\n",
    "GitHub homepage: https://github.com/traveller59/spconv\n",
    "### 1.2 LocalZO.conv_models\n",
    "Implementations of Spike Neurons (So far, only LIF), that can glue between Spconv layers\n",
    "\n",
    "## 2 Training the Spconv\n",
    "### 2.1 Data loading\n",
    "first, let's load the data using tonic. Details can be found in train_with_torch_nuro.ipynb, so we just skip it here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size: 60000 testset size: 10000 type of dataset <class 'numpy.ndarray'> shape (298, 2, 34, 34)\n"
     ]
    }
   ],
   "source": [
    "# set the gpu device\n",
    "gpu_idx = '3'\n",
    "import os\n",
    "from tonic.datasets import NMNIST\n",
    "import tonic\n",
    "from tonic import transforms\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_idx\n",
    "\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "# remove isolated events\n",
    "# sum a period of events into a frame\n",
    "frame_transform = transforms.Compose([\n",
    "    transforms.Denoise(filter_time=10000),\n",
    "    transforms.ToFrame(sensor_size=sensor_size, time_window=1000),\n",
    "])  # the output of ToFrame is a tuple of (frame, label), where frame is np.ndarray\n",
    "\n",
    "trainset = tonic.datasets.NMNIST(save_to='/home/zxh/data', train=True, transform=frame_transform)\n",
    "testset = tonic.datasets.NMNIST(save_to='/home/zxh/data', train=False, transform=frame_transform)\n",
    "print('trainset size:', len(trainset), 'testset size:', len(testset), 'type of dataset', type(trainset[0][0]), 'shape', trainset[0][0].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Define the network\n",
    "The construction of a Spconv network is basically same with constructing any torch.nn networks, except for a few things to notice:\n",
    "* The computation is time-first, and at the end of LIF layer we reshape the input into [time*batch, *inputs] before feeding into spconv network to improve the performance. So, we need to reshape it back before output.\n",
    "* The input of a Spconv network is a sparse tensor, constructed by spconv.SparseConvTensor.from_dense()\n",
    "* The LeakyPlain (implemented in LocalZO.conv_models.neurons) is a spike neuron that can glue between Spconv layers. It needs to specify the batch_size, u_th and beta. Batch size is used to reshape input and output, u_th and beta are parameters of LIF neuron.\n",
    "* The output should be a tensor with shape (time, batch, *inputs).\n",
    "\n",
    "**Important:** The spconv library treat empty input (i.e. input with only zeros) as an error\n",
    "Usually this will not happen, if so, we will get *CUDA kernel launch blocks must be positive, but got N= 0*. and please set appropriate u_th and beta to make at least one spike fire during the time stamps. Details can be found in : https://github.com/traveller59/spconv/blob/master/docs/COMMON_PROBLEMS.md"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExampleNet(\n",
      "  (conv_block1): Sequential(\n",
      "    (0): SparseConv2d(2, 16, kernel_size=[5, 5], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.Native)\n",
      "    (1): SparseBatchNorm(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SparseMaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=[0, 0], dilation=[1, 1], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (3): LeakyPlain()\n",
      "  )\n",
      "  (conv_block2): Sequential(\n",
      "    (0): SparseConv2d(16, 32, kernel_size=[5, 5], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.Native)\n",
      "    (1): SparseBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SparseMaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=[0, 0], dilation=[1, 1], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (3): LeakyPlain()\n",
      "  )\n",
      "  (to_dense): ToDense()\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=800, out_features=10, bias=True)\n",
      "    (1): LeakyPlain()\n",
      "  )\n",
      ")\n",
      "conv_block1.0.weight torch.Size([16, 5, 5, 2])\n",
      "conv_block1.0.bias torch.Size([16])\n",
      "conv_block1.1.weight torch.Size([16])\n",
      "conv_block1.1.bias torch.Size([16])\n",
      "conv_block2.0.weight torch.Size([32, 5, 5, 16])\n",
      "conv_block2.0.bias torch.Size([32])\n",
      "conv_block2.1.weight torch.Size([32])\n",
      "conv_block2.1.bias torch.Size([32])\n",
      "fc.0.weight torch.Size([10, 800])\n",
      "fc.0.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from spconv import pytorch as spconv  # pay attention to the spconv.pytorch\n",
    "from conv_models.neurons import LeakyPlain\n",
    "\n",
    "\n",
    "class ExampleNet(nn.Module):\n",
    "    def __init__(self, batch_size, u_th, beta, conv_algorithm = spconv.ConvAlgo.Native):\n",
    "        super(ExampleNet, self).__init__()\n",
    "        self.batch_size= batch_size\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            spconv.SparseConv2d(2, 16, 5, bias=True, algo=conv_algorithm),\n",
    "            spconv.SparseBatchNorm(16, eps=1e-5, momentum=0.1),\n",
    "            spconv.SparseMaxPool2d(2, stride=2),\n",
    "            LeakyPlain(u_th=u_th, beta=beta, batch_size=batch_size),\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            spconv.SparseConv2d(16, 32, 5, bias=True, algo=conv_algorithm),\n",
    "            spconv.SparseBatchNorm(32, eps=1e-5, momentum=0.1),\n",
    "            spconv.SparseMaxPool2d(2, stride=2),\n",
    "            LeakyPlain(u_th=u_th, beta=beta, batch_size=batch_size),\n",
    "        )\n",
    "        self.to_dense = spconv.ToDense() # convert the sparse tensor to dense tensor\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32*5*5, 10),\n",
    "            LeakyPlain(u_th=u_th, beta=beta, batch_size=batch_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert isinstance(x, spconv.SparseConvTensor)\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.to_dense(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.batch_size, 10)  # reshape back into time, batch, *inputs\n",
    "        return x\n",
    "\n",
    "batch_size = 128\n",
    "net = ExampleNet(batch_size=batch_size, u_th=0.8, beta=0.6).cuda()\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Define the loss function and optimizer\n",
    "we use loss function from snntorch, and optimizer from torch.optim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from snntorch import functional as SF\n",
    "import torch\n",
    "\n",
    "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "acc_fn = SF.accuracy_rate\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Cache dataset to accelerate training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cache_transform = tonic.transforms.Compose([\n",
    "    torch.from_numpy,\n",
    "    torchvision.transforms.RandomRotation([-10,10]),\n",
    "])\n",
    "\n",
    "epoch_num = 5\n",
    "cached_trainset = tonic.DiskCachedDataset(trainset, cache_path='./data/cache', transform=cache_transform)\n",
    "cached_testset = tonic.DiskCachedDataset(testset, cache_path='./data/cache',)\n",
    "\n",
    "train_loader = DataLoader(cached_trainset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "test_loader = DataLoader(cached_testset, batch_size=batch_size, shuffle=False, num_workers=4, drop_last=True,\n",
    "                         collate_fn=tonic.collation.PadTensors(batch_first=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.6 Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "#\n",
    "# with SummaryWriter(comment='spconv', log_dir='./output/spconv') as writer:\n",
    "#     for epoch in range(epoch_num):\n",
    "#         for i, (inputs, labels) in enumerate(train_loader):\n",
    "#             inputs = inputs.view(-1, *inputs.shape[2:]).transpose(1, 3).cuda()\n",
    "#             inputs = spconv.SparseConvTensor.from_dense(inputs)\n",
    "#             labels = labels.cuda()\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = net(inputs)\n",
    "#             loss = loss_fn(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#\n",
    "#             if i % 100 == 0:\n",
    "#                 writer.add_scalar('loss', loss.item(), epoch * len(train_loader) + i)\n",
    "#                 writer.add_scalar('acc', acc_fn(outputs, labels).item(), epoch * len(train_loader) + i)\n",
    "#                 print('epoch', epoch, 'batch', i, 'loss', loss.item(), 'acc', acc_fn(outputs, labels).item())\n",
    "#\n",
    "#         acc = 0\n",
    "#         loss_val = 0\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs = inputs.cuda()\n",
    "#             labels = labels.cuda()\n",
    "#             outputs = net(inputs)\n",
    "#             loss_val += loss_fn(outputs, labels).item()\n",
    "#             acc += acc_fn(outputs, labels).item()\n",
    "#\n",
    "#         writer.add_scalar('test loss', loss_val/len(test_loader), epoch)\n",
    "#         writer.add_scalar('test acc', acc/len(test_loader), epoch)\n",
    "#         print('epoch', epoch, 'test loss', loss_val/len(test_loader), 'test acc', acc/len(test_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Training with LocalZO neurons\n",
    "### 3.1 Implementation Details\n",
    "The LocalZO neurons use random sampler to generate random tangents to approximate gradient\n",
    "Since the sampling technique varies, to instantiate a LocalZo neuron, we need to provide a sampler. For customized sampler, we recommend to inherit the base class `BaseSampler` and implement the `generate_random_tangents` method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExampleNetLocalZO(\n",
      "  (conv_block1): Sequential(\n",
      "    (0): SparseConv2d(2, 16, kernel_size=[5, 5], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.Native)\n",
      "    (1): SparseBatchNorm(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SparseMaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=[0, 0], dilation=[1, 1], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (3): LeakeyZOPlain()\n",
      "  )\n",
      "  (conv_block2): Sequential(\n",
      "    (0): SparseConv2d(16, 32, kernel_size=[5, 5], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.Native)\n",
      "    (1): SparseBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SparseMaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=[0, 0], dilation=[1, 1], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (3): LeakeyZOPlain()\n",
      "  )\n",
      "  (to_dense): ToDense()\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=800, out_features=10, bias=True)\n",
      "    (1): LeakyPlain()\n",
      "  )\n",
      ")\n",
      "conv_block1.0.weight torch.Size([16, 5, 5, 2])\n",
      "conv_block1.0.bias torch.Size([16])\n",
      "conv_block1.1.weight torch.Size([16])\n",
      "conv_block1.1.bias torch.Size([16])\n",
      "conv_block2.0.weight torch.Size([32, 5, 5, 16])\n",
      "conv_block2.0.bias torch.Size([32])\n",
      "conv_block2.1.weight torch.Size([32])\n",
      "conv_block2.1.bias torch.Size([32])\n",
      "fc.0.weight torch.Size([10, 800])\n",
      "fc.0.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "from conv_models.neurons import LeakeyZOPlain\n",
    "from conv_models.samplers import NormalSampler, BaseSampler\n",
    "\n",
    "\n",
    "class ExampleNetLocalZO(nn.Module):\n",
    "    def __init__(self, batch_size, u_th, beta, conv_algorithm = spconv.ConvAlgo.Native, random_sampler: BaseSampler=NormalSampler, sample_num=1):\n",
    "        super(ExampleNetLocalZO, self).__init__()\n",
    "        self.batch_size= batch_size\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            spconv.SparseConv2d(2, 16, 5, bias=True, algo=conv_algorithm),\n",
    "            spconv.SparseBatchNorm(16, eps=1e-5, momentum=0.1),\n",
    "            spconv.SparseMaxPool2d(2, stride=2),\n",
    "            LeakeyZOPlain(u_th=u_th, beta=beta, batch_size=batch_size, random_sampler=random_sampler(),sample_num=sample_num)\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            spconv.SparseConv2d(16, 32, 5, bias=True, algo=conv_algorithm),\n",
    "            spconv.SparseBatchNorm(32, eps=1e-5, momentum=0.1),\n",
    "            spconv.SparseMaxPool2d(2, stride=2),\n",
    "            LeakeyZOPlain(u_th=u_th, beta=beta, batch_size=batch_size, random_sampler=random_sampler(),sample_num=sample_num)\n",
    "        )\n",
    "        self.to_dense = spconv.ToDense() # convert the sparse tensor to dense tensor\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32*5*5, 10),\n",
    "            LeakyPlain(u_th=u_th, beta=beta, batch_size=batch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert isinstance(x, spconv.SparseConvTensor)\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.to_dense(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.batch_size, 10)  # reshape back into time, batch, *inputs\n",
    "        return x\n",
    "\n",
    "net = ExampleNetLocalZO(batch_size=batch_size, u_th=1.0, beta=0.5).cuda()\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 batch 0 loss 28.102920532226562 acc 0.0859375 time 1.9416148662567139\n",
      "epoch 0 batch 10 loss 10.653488159179688 acc 0.2578125 time 2.493873357772827\n",
      "epoch 0 batch 20 loss 9.240630149841309 acc 0.3828125 time 1.2283501625061035\n",
      "epoch 0 batch 30 loss 8.502413749694824 acc 0.453125 time 1.212583303451538\n",
      "epoch 0 batch 40 loss 7.541042327880859 acc 0.6328125 time 1.1968114376068115\n",
      "epoch 0 batch 50 loss 7.622472763061523 acc 0.5859375 time 1.4092624187469482\n",
      "epoch 0 batch 60 loss 7.009034633636475 acc 0.6796875 time 1.1928646564483643\n",
      "epoch 0 batch 70 loss 6.824457168579102 acc 0.703125 time 1.255183458328247\n",
      "epoch 0 batch 80 loss 6.96734094619751 acc 0.6328125 time 2.0540895462036133\n",
      "epoch 0 batch 90 loss 6.44566011428833 acc 0.71875 time 1.5420308113098145\n",
      "epoch 0 batch 100 loss 7.009382247924805 acc 0.6015625 time 2.4582467079162598\n",
      "epoch 0 batch 110 loss 6.407059192657471 acc 0.7109375 time 1.254889965057373\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     16\u001B[0m labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m     17\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 18\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(outputs, labels)\n\u001B[1;32m     20\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/.conda/envs/xiaohan/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[0;32mIn [17]\u001B[0m, in \u001B[0;36mExampleNetLocalZO.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, spconv\u001B[38;5;241m.\u001B[39mSparseConvTensor)\n\u001B[1;32m     30\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv_block1(x)\n\u001B[0;32m---> 31\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv_block2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_dense(x)\n\u001B[1;32m     33\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mflatten(x)\n",
      "File \u001B[0;32m~/.conda/envs/xiaohan/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.conda/envs/xiaohan/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/.conda/envs/xiaohan/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/remote/sparse_zo_nips2023/LocalZO/conv_models/neurons.py:73\u001B[0m, in \u001B[0;36mLeakeyZOPlain.forward\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     70\u001B[0m inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mdense()\n\u001B[1;32m     71\u001B[0m random_tangents \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandom_sampler\u001B[38;5;241m.\u001B[39mgenerate_random_tangents(inputs\u001B[38;5;241m.\u001B[39mshape, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msample_num,\n\u001B[1;32m     72\u001B[0m                                                                device\u001B[38;5;241m=\u001B[39minputs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m---> 73\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mPlainLIFLocalZO\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mu_th\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbeta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_tangents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdelta\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m spconv\u001B[38;5;241m.\u001B[39mSparseConvTensor\u001B[38;5;241m.\u001B[39mfrom_dense(outputs\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m))\n",
      "File \u001B[0;32m~/.conda/envs/xiaohan/lib/python3.8/site-packages/torch/autograd/function.py:506\u001B[0m, in \u001B[0;36mFunction.apply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[1;32m    504\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[1;32m    505\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[0;32m--> 506\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39msetup_context \u001B[38;5;241m==\u001B[39m _SingleLevelFunction\u001B[38;5;241m.\u001B[39msetup_context:\n\u001B[1;32m    509\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    510\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    511\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    512\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/remote/sparse_zo_nips2023/LocalZO/conv_models/functional.py:85\u001B[0m, in \u001B[0;36mPlainLIFLocalZO.forward\u001B[0;34m(ctx, inputs, batch_size, u_th, beta, random_tangents, delta)\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;66;03m# the shape of random_tangent is [sample_num, batch_size, *input_size]\u001B[39;00m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;66;03m# providing mask for condition |u| < |z|delta in the paper\u001B[39;00m\n\u001B[1;32m     84\u001B[0m g_2_mask \u001B[38;5;241m=\u001B[39m (torch\u001B[38;5;241m.\u001B[39mabs(current_memberance_potential \u001B[38;5;241m-\u001B[39m u_th) \u001B[38;5;241m<\u001B[39m torch\u001B[38;5;241m.\u001B[39mabs(random_tangent) \u001B[38;5;241m*\u001B[39m delta)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m---> 85\u001B[0m g_2 \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mabs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrandom_tangent\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m/\u001B[39m delta \u001B[38;5;241m*\u001B[39m g_2_mask\n\u001B[1;32m     86\u001B[0m grad_heaviside \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmean(g_2, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     87\u001B[0m grad_heavisides\u001B[38;5;241m.\u001B[39mappend(grad_heaviside)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from snntorch import functional as SF\n",
    "import torch\n",
    "\n",
    "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "acc_fn = SF.accuracy_rate\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3)\n",
    "\n",
    "with SummaryWriter(comment='spconv', log_dir='./output/spconv') as writer:\n",
    "    for epoch in range(epoch_num):\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            start = time.time()\n",
    "            inputs = inputs.view(-1, *inputs.shape[2:]).transpose(1, 3).cuda()\n",
    "            inputs = spconv.SparseConvTensor.from_dense(inputs)\n",
    "            labels = labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                writer.add_scalar('loss', loss.item(), epoch * len(train_loader) + i)\n",
    "                writer.add_scalar('acc', acc_fn(outputs, labels).item(), epoch * len(train_loader) + i)\n",
    "                print('epoch', epoch, 'batch', i, 'loss', loss.item(), 'acc', acc_fn(outputs, labels).item(), 'time', end-start)\n",
    "\n",
    "        acc = 0\n",
    "        loss_val = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = net(inputs)\n",
    "            loss_val += loss_fn(outputs, labels).item()\n",
    "            acc += acc_fn(outputs, labels).item()\n",
    "\n",
    "        writer.add_scalar('test loss', loss_val/len(test_loader), epoch)\n",
    "        writer.add_scalar('test acc', acc/len(test_loader), epoch)\n",
    "        print('epoch', epoch, 'test loss', loss_val/len(test_loader), 'test acc', acc/len(test_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-.conda-xiaohan-py",
   "language": "python",
   "display_name": "Python [conda env:.conda-xiaohan] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}